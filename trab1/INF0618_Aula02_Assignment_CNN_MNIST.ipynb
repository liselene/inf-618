{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 01 - Exploring CNNs for MNIST\n",
    "\n",
    "In this assignment, we want you to modify the CNN architecture that we used in the last MNIST exercise, adding new layers and altering their hyperparameters. We have already loaded and preprocessed the data for you, so you can focus on the architecture and training of your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "*** You do not need to alter the blocks `Imports`, `Load training data` and `Preprocessing` ***\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import shuffle, seed\n",
    "seed(42)  #keep this seed, in order to compare the results with your classmates\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (15,15) # Make the figures a bit bigger\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "\n",
    "# the data, shuffled and split between trainVal and test sets\n",
    "(trainVal_data, trainVal_label), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# We want now to split the trainVal data into train and validation sets\n",
    "nData = trainVal_data.shape[0]  #find the size of trainVal\n",
    "nTrain = int(nData * 0.8)  #80% to train, 20% to val\n",
    "\n",
    "randomIdx = list(range(nData))   #randomly select indexes\n",
    "shuffle(randomIdx)\n",
    "trainIdx = randomIdx[:nTrain] \n",
    "valIdx = randomIdx[nTrain:]\n",
    "\n",
    "# Split the data\n",
    "X_val, y_val = trainVal_data[valIdx], trainVal_label[valIdx]\n",
    "X_train, y_train = trainVal_data[trainIdx], trainVal_label[trainIdx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain class weights and samples per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train --->  {0: 4724, 1: 5393, 2: 4723, 3: 4881, 4: 4704, 5: 4313, 6: 4769, 7: 5001, 8: 4730, 9: 4762} \n",
      "\n",
      "{0: 1.0160880609652836, 1: 0.8900426478768775, 2: 1.0163031971204743, 3: 0.9834050399508297, 4: 1.0204081632653061, 5: 1.1129144447020636, 6: 1.0065003145313483, 7: 0.9598080383923215, 8: 1.014799154334038, 9: 1.0079798404031919} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Train ---> \", dict(zip(unique, counts)), \"\\n\")\n",
    "\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "train_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "print(train_class_weights, \"\\n\")\n",
    "\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "test_sample_per_class = counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training matrix shape (48000, 28, 28, 1)\n",
      "Validation matrix shape (12000, 28, 28, 1)\n",
      "Testing matrix shape (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "#The first dimension refers to the number of images\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_val = X_val.reshape(X_val.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_val /= 255\n",
    "X_test /= 255\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_val = np_utils.to_categorical(y_val, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "print(\"Training matrix shape\", X_train.shape)\n",
    "print(\"Validation matrix shape\", X_val.shape)\n",
    "print(\"Testing matrix shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------\n",
    "------------------------------------------\n",
    "------------------------------------------\n",
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Your assignment starts here!!!***\n",
    "\n",
    "# Task 1 [0.25 pts] - Add a fully-connected layer\n",
    "Let's investigate if the network gets better as we add more layers to it. We want you to add another fully-connected layer to the network from last exercise. Your network will have:\n",
    "- Convolutional layer with 10 filters of size 5x5 with ReLU activation;\n",
    "- Max pooling layer with kernel 2x2, that will reduce each spatial dimension by half (24x24 to 12x12);\n",
    "- An operation to flatten the feature maps into an array of size 10x12x12 = 1440\n",
    "- Dropout operation with probability 0.25, applied to flattened array\n",
    "- **Fully connected layer with 100 units/neurons and ReLU activation;**\n",
    "- Fully connected layer with units/neurons equal to the number of classes in our problem (in this case, 10);\n",
    "- Softmax activation on the last FC layer.\n",
    "\n",
    "The architecture will look like this:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"task1_cnn.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your model here\n",
    "model = Sequential()\n",
    "\n",
    "#Conv layer with 10 filters of size 5x5 and ReLU activation\n",
    "model.add(Conv2D(10, kernel_size=(5,5), activation='relu', input_shape=(28,28,1)))\n",
    "\n",
    "#Max pooling of size 2x2\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Flatten operation\n",
    "model.add(Flatten())\n",
    "\n",
    "#Dropout with probability 0.25\n",
    "model.add(Dropout(0,25))\n",
    "\n",
    "#FC layer with 100 neurons and softmax activation\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "#FC layer with 10 neurons and softmax activation\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/1\n",
      "48000/48000 [==============================] - 17s 347us/step - loss: 1.1360 - acc: 0.6931 - val_loss: 0.4379 - val_acc: 0.8842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efd859d1208>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and train your model\n",
    "sgd = optimizers.SGD(lr=0.01) #lr = learning rate\n",
    "model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=128, epochs=1, verbose=1,\n",
    "          class_weight = train_class_weights,\n",
    "          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 194us/step\n",
      "Test loss: 0.41064972746372225\n",
      "Test accuracy (NOT NORMALIZED): 0.8938\n",
      "{0: 0.9663265306122449, 1: 0.9629955947136564, 2: 0.877906976744186, 3: 0.8861386138613861, 4: 0.9103869653767821, 5: 0.8105381165919282, 6: 0.9227557411273486, 7: 0.872568093385214, 8: 0.8326488706365504, 9: 0.8800792864222002} \n",
      "\n",
      "Normalized Acc -->  0.8922344789471497\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy (NOT NORMALIZED):', score[1])\n",
    "\n",
    "\n",
    "predicted_classes = model.predict_classes(X_test)\n",
    "Y_test_classes = np.argmax(Y_test, axis=-1)\n",
    "\n",
    "accPerClass = []\n",
    "for classIdx in range(nb_classes):\n",
    "    idx = (Y_test_classes == classIdx)\n",
    "    \n",
    "    correctPred = np.sum(predicted_classes[idx] == Y_test_classes[idx])\n",
    "    accPerClass.append( correctPred / float(test_sample_per_class[classIdx]))\n",
    "    \n",
    "print(dict(zip(range(nb_classes),accPerClass)), \"\\n\")\n",
    "print(\"Normalized Acc --> \", np.mean(accPerClass))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "----------------\n",
    "# Task 2 [0.25 pts] - Add another (conv + max pooling) layers\n",
    "We want you to add another convolutional layer, followed by a max pooling to the network. Your network will have:\n",
    "- Convolutional layer with 10 filters of size 5x5 with ReLU activation;\n",
    "- Max pooling layer with kernel 2x2, that will reduce each spatial dimension by half (24x24 to 12x12);\n",
    "- **Convolutional layer with 20 filters of size 5x5 with ReLU activation;**\n",
    "- **Max pooling layer with kernel 2x2, that will reduce each spatial dimension by half (8x8 to 4x4);**\n",
    "- An operation to flatten the feature maps into an array of size 20x4x4 = 320\n",
    "- Dropout operation with probability 0.25, applied to flattened array\n",
    "- Fully connected layer with units/neurons equal to the number of classes in our problem (in this case, 10);\n",
    "- Softmax activation on the last FC layer.\n",
    "\n",
    "The architecture will look like this:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"task2_cnn.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your model here\n",
    "model = Sequential()\n",
    "\n",
    "#Conv layer with 10 filters of size 5x5 and ReLU activation\n",
    "model.add(Conv2D(10, kernel_size=(5, 5),\n",
    "activation='relu', #ReLU activation\n",
    "input_shape=(28, 28, 1))) #no need to include the batch size\n",
    "\n",
    "#Max pooling of size 2x2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#Conv layer with 20 filters of size 5x5 and ReLU activation\n",
    "model.add(Conv2D(20, kernel_size=(5, 5),\n",
    "activation='relu', #ReLU activation\n",
    "input_shape=(12, 12, 1))) #no need to include the batch size\n",
    "\n",
    "#Max pooling of size 2x2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "\n",
    "#Flatten operation\n",
    "model.add(Flatten())\n",
    "\n",
    "#Dropout with probability 0.25\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#FC layer with 10 neurons and softmax activation\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/1\n",
      "48000/48000 [==============================] - 29s 595us/step - loss: 1.4023 - acc: 0.5488 - val_loss: 0.5009 - val_acc: 0.8692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efdb0d99438>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and train your model\n",
    "sgd = optimizers.SGD(lr=0.01) #lr = learning rate\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=128, epochs=1, verbose=1,\n",
    "          class_weight = train_class_weights,\n",
    "          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 304us/step\n",
      "Test loss: 0.4697973629236221\n",
      "Test accuracy (NOT NORMALIZED): 0.8763\n",
      "{0: 0.9612244897959183, 1: 0.9797356828193833, 2: 0.873062015503876, 3: 0.8544554455445544, 4: 0.7922606924643585, 5: 0.8038116591928252, 6: 0.9175365344467641, 7: 0.853112840466926, 8: 0.8203285420944558, 9: 0.8870168483647175} \n",
      "\n",
      "Normalized Acc -->  0.8742544750693778\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy (NOT NORMALIZED):', score[1])\n",
    "\n",
    "\n",
    "predicted_classes = model.predict_classes(X_test)\n",
    "Y_test_classes = np.argmax(Y_test, axis=-1)\n",
    "\n",
    "accPerClass = []\n",
    "for classIdx in range(nb_classes):\n",
    "    idx = (Y_test_classes == classIdx)\n",
    "    \n",
    "    correctPred = np.sum(predicted_classes[idx] == Y_test_classes[idx])\n",
    "    accPerClass.append( correctPred / float(test_sample_per_class[classIdx]))\n",
    "    \n",
    "print(dict(zip(range(nb_classes),accPerClass)), \"\\n\")\n",
    "print(\"Normalized Acc --> \", np.mean(accPerClass))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "----------------\n",
    "# Task 3 [0.25 pts] - Add Conv + MaxPool + FC\n",
    "Now combine tasks 1 and 2; adding the convolutional, max pooling and fc layer to your network. Your CNN will have:\n",
    "- Convolutional layer with 10 filters of size 5x5 with ReLU activation;\n",
    "- Max pooling layer with kernel 2x2, that will reduce each spatial dimension by half (24x24 to 12x12);\n",
    "- **Convolutional layer with 20 filters of size 5x5 with ReLU activation;**\n",
    "- **Max pooling layer with kernel 2x2, that will reduce each spatial dimension by half (8x8 to 4x4);**\n",
    "- An operation to flatten the feature maps into an array of size 20x4x4 = 320\n",
    "- Dropout operation with probability 0.25, applied to flattened array\n",
    "- **Fully connected layer with 100 units/neurons and ReLU activation;**\n",
    "- Fully connected layer with units/neurons equal to the number of classes in our problem (in this case, 10);\n",
    "- Softmax activation on the last FC layer.\n",
    "\n",
    "The architecture will look like this:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"task3_cnn.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your model here\n",
    "model = Sequential()\n",
    "\n",
    "#Conv layer with 10 filters of size 5x5 and ReLU activation\n",
    "model.add(Conv2D(10, kernel_size=(5, 5),\n",
    "activation='relu', #ReLU activation\n",
    "input_shape=(28, 28, 1))) #no need to include the batch size\n",
    "\n",
    "#Max pooling of size 2x2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#Conv layer with 20 filters of size 5x5 and ReLU activation\n",
    "model.add(Conv2D(20, kernel_size=(5, 5),\n",
    "activation='relu', #ReLU activation\n",
    "input_shape=(12, 12, 1))) #no need to include the batch size\n",
    "\n",
    "#Max pooling of size 2x2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "\n",
    "#Flatten operation\n",
    "model.add(Flatten())\n",
    "\n",
    "#Dropout with probability 0.25\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#FC layer with 100 neurons and softmax activation\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "#FC layer with 10 neurons and softmax activation\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/1\n",
      "48000/48000 [==============================] - 31s 655us/step - loss: 1.6754 - acc: 0.4425 - val_loss: 0.6317 - val_acc: 0.8275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efd8ee067f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and train your model\n",
    "sgd = optimizers.SGD(lr=0.01) #lr = learning rate\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=128, epochs=1, verbose=1,\n",
    "          class_weight = train_class_weights,\n",
    "          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 328us/step\n",
      "Test loss: 0.6135878682613373\n",
      "Test accuracy (NOT NORMALIZED): 0.841\n",
      "{0: 0.9479591836734694, 1: 0.9568281938325991, 2: 0.8333333333333334, 3: 0.8594059405940594, 4: 0.7759674134419552, 5: 0.7746636771300448, 6: 0.802713987473904, 7: 0.8171206225680934, 8: 0.7792607802874744, 9: 0.8384539147670962} \n",
      "\n",
      "Normalized Acc -->  0.8385707047102029\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy (NOT NORMALIZED):', score[1])\n",
    "\n",
    "\n",
    "predicted_classes = model.predict_classes(X_test)\n",
    "Y_test_classes = np.argmax(Y_test, axis=-1)\n",
    "\n",
    "accPerClass = []\n",
    "for classIdx in range(nb_classes):\n",
    "    idx = (Y_test_classes == classIdx)\n",
    "    \n",
    "    correctPred = np.sum(predicted_classes[idx] == Y_test_classes[idx])\n",
    "    accPerClass.append( correctPred / float(test_sample_per_class[classIdx]))\n",
    "    \n",
    "print(dict(zip(range(nb_classes),accPerClass)), \"\\n\")\n",
    "print(\"Normalized Acc --> \", np.mean(accPerClass))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "----------------\n",
    "# Task 4 [0.25 pts] - Implement a new modification\n",
    "Implement **one** modification to your network and evaluate it. Some possible alterations are:\n",
    "- Add more convolutional and/or max pooling layers;\n",
    "- Alter the kernel size and number of filters of the conv layers;\n",
    "- Try training with different batch sizes and higher number of epochs;\n",
    "- Try with different activations, besides ReLU and Softmax;\n",
    "- Try optimizing the CNN with a different loss;\n",
    "- Try different learning rates;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/12\n",
      "48000/48000 [==============================] - 16s 341us/step - loss: 1.2159 - acc: 0.6859 - val_loss: 0.5032 - val_acc: 0.8604\n",
      "Epoch 2/12\n",
      "48000/48000 [==============================] - 17s 351us/step - loss: 0.4293 - acc: 0.8797 - val_loss: 0.3872 - val_acc: 0.8885\n",
      "Epoch 3/12\n",
      "48000/48000 [==============================] - 21s 429us/step - loss: 0.3644 - acc: 0.8961 - val_loss: 0.3517 - val_acc: 0.8963\n",
      "Epoch 4/12\n",
      "48000/48000 [==============================] - 20s 414us/step - loss: 0.3372 - acc: 0.9031 - val_loss: 0.3348 - val_acc: 0.9011\n",
      "Epoch 5/12\n",
      "48000/48000 [==============================] - 22s 452us/step - loss: 0.3191 - acc: 0.9086 - val_loss: 0.3165 - val_acc: 0.9085\n",
      "Epoch 6/12\n",
      "48000/48000 [==============================] - 19s 389us/step - loss: 0.3052 - acc: 0.9131 - val_loss: 0.3073 - val_acc: 0.9138\n",
      "Epoch 7/12\n",
      "48000/48000 [==============================] - 18s 377us/step - loss: 0.2929 - acc: 0.9163 - val_loss: 0.3034 - val_acc: 0.9123\n",
      "Epoch 8/12\n",
      "48000/48000 [==============================] - 18s 367us/step - loss: 0.2812 - acc: 0.9202 - val_loss: 0.2865 - val_acc: 0.9190\n",
      "Epoch 9/12\n",
      "48000/48000 [==============================] - 20s 422us/step - loss: 0.2702 - acc: 0.9233 - val_loss: 0.2774 - val_acc: 0.9221\n",
      "Epoch 10/12\n",
      "48000/48000 [==============================] - 18s 383us/step - loss: 0.2591 - acc: 0.9266 - val_loss: 0.2650 - val_acc: 0.9253\n",
      "Epoch 11/12\n",
      "48000/48000 [==============================] - 18s 375us/step - loss: 0.2485 - acc: 0.9299 - val_loss: 0.2541 - val_acc: 0.9291\n",
      "Epoch 12/12\n",
      "48000/48000 [==============================] - 18s 383us/step - loss: 0.2383 - acc: 0.9326 - val_loss: 0.2459 - val_acc: 0.9318\n",
      "10000/10000 [==============================] - 2s 245us/step\n",
      "Test loss: 0.22696970573067665\n",
      "Test accuracy (NOT NORMALIZED): 0.9333\n",
      "{0: 0.9826530612244898, 1: 0.9823788546255506, 2: 0.9166666666666666, 3: 0.9168316831683169, 4: 0.9572301425661914, 5: 0.9192825112107623, 6: 0.9467640918580376, 7: 0.9173151750972762, 8: 0.8901437371663244, 9: 0.8979187314172448} \n",
      "\n",
      "Normalized Acc -->  0.932718465500086\n"
     ]
    }
   ],
   "source": [
    "# define your model here\n",
    "model = Sequential()\n",
    "\n",
    "#Conv layer with 10 filters of size 5x5 and ReLU activation\n",
    "model.add(Conv2D(10, kernel_size=(5,5), activation='relu', input_shape=(28,28,1)))\n",
    "\n",
    "#Max pooling of size 2x2\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Flatten operation\n",
    "model.add(Flatten())\n",
    "\n",
    "#Dropout with probability 0.25\n",
    "model.add(Dropout(0,25))\n",
    "\n",
    "#FC layer with 10 neurons and softmax activation\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# compile and train your model\n",
    "sgd = optimizers.SGD(lr=0.01) #lr = learning rate\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=128, epochs=12, verbose=1,\n",
    "          class_weight = train_class_weights,\n",
    "          validation_data=(X_val, Y_val))\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy (NOT NORMALIZED):', score[1])\n",
    "\n",
    "predicted_classes = model.predict_classes(X_test)\n",
    "Y_test_classes = np.argmax(Y_test, axis=-1)\n",
    "\n",
    "accPerClass = []\n",
    "for classIdx in range(nb_classes):\n",
    "    idx = (Y_test_classes == classIdx)\n",
    "    \n",
    "    correctPred = np.sum(predicted_classes[idx] == Y_test_classes[idx])\n",
    "    accPerClass.append( correctPred / float(test_sample_per_class[classIdx]))\n",
    "    \n",
    "print(dict(zip(range(nb_classes),accPerClass)), \"\\n\")\n",
    "print(\"Normalized Acc --> \", np.mean(accPerClass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
